---
title: "M√©todo de descenso de gradiente en regresi√≥n"
author: "Alicia Jiajun Lorenzo, Abraham Trashorras "
date: "2023-10-16"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1. Simular una muestra de tama√±o n = 100 de valores de (xi, yi), i ‚àà {1, . . . , n}.
a) Para ello, se pueden generar 100 observaciones xi de la uniforme (runif).

```{r}
n <- 100
xi <- runif(n)  # Genera 100 observaciones xi de la uniforme

```

b) Los errores del modelo, i se pueden generar de la distribuci√≥n normal de media 0 y va-
rianza œÉ2. Por ejemplo, si cogemos œÉ2 = 1, esto se puede hacer con (rnorm(100,0,1)).

```{r}
sigma <- 1      # Variabilidad de los errores
ei <- rnorm(n, 0, sigma)  # Genera errores de la distribuci√≥n normal
```

c) Los valores de yi se calcular√≠an como yi = Œ≤0 + Œ≤1xi + i.
```{r}
beta0<-5 # supongamos que betha0=5
beta1<-3 # suponngamos que beta1=3
yi <- beta0 + beta1 * xi + ei  # Calcula los valores de yi
```

2. Escoger la funci√≥n a minimizar. En este caso, la suma de los residuos al cuadrado:

  J(Œ≤0, Œ≤1) =‚àë(yi ‚àí Œ≤0 ‚àí Œ≤1xi)¬≤.

```{r cars}

suma_residuos_cuadrado <- function(beta0, beta1) {
  sum((yi - beta0 - beta1 * xi)^2)
}

```

3. Obtener las derivadas parciales de la funci√≥n a minimizar. A partir de estas, aplicad el algoritmo de m√©todo de descenso gradiente (v√©ase los apuntes del Tema de Optimizaci√≥n convexa). 

En la Secci√≥n 2.1 de la pr√°ctica de Modelos de regresi√≥n lineal con R se dan los pasos a se-
guir para el caso del modelo de regresi√≥n lineal simple


```{r pressure, echo=FALSE}

library(ggplot2)
library(gganimate)
# Aplicar el descenso de gradiente
learning_rate <- 0.01
num_iterations <- 1000
beta0 <- 0
beta1 <- 0


# Crear un dataframe para almacenar los resultados de cada iteraci√≥n
results <- data.frame(iteration = numeric(0), beta0 = numeric(0), beta1 = numeric(0), J = numeric(0))

# Realizar el descenso de gradiente y almacenar los resultados
for (iteration in 1:num_iterations) {
  dJ_beta0 <- -2 * sum(yi - beta0 - beta1 * xi)
  dJ_beta1 <- -2 * sum((yi - beta0 - beta1 * xi) * xi)
  beta0 <- beta0 - learning_rate * dJ_beta0
  beta1 <- beta1 - learning_rate * dJ_beta1
  J_value <- suma_residuos_cuadrado(beta0, beta1)
  results <- rbind(results, data.frame(iteration = iteration, beta0 = beta0, beta1 = beta1, J = J_value))
}
```

Crea una animaci√≥n utilizando ggplot2 y gganimate. Puedes utilizar un gr√°fico de dispersi√≥n para visualizar la convergencia del descenso de gradiente en cada iteraci√≥n.

```{r}
p <- ggplot(results, aes(x = beta0, y = beta1)) +
  geom_point(aes(color = J), size = 2) +
  scale_color_gradient(low = "blue", high = "red") +
  labs(x = expression(beta[0]), y = expression(beta[1]), title = "Descenso del Gradiente") +
  transition_states(iteration, transition_length = 2, state_length = 1) +
  enter_fade() +
  exit_fade()

# Guardar la animaci√≥n en un archivo
anim_save("descenso_gradiente.gif", p)
```

4. Como en este caso, tenemos disponible la soluci√≥n √≥ptima a este problema en la funci√≥n lm. Podemos repetir (en un bucle) varias veces los pasos anteriores y hacer una comparativa con los estimadores ( ÀÜŒ≤0 y ÀÜŒ≤1) que devuelve dicha funci√≥n. Por ejemplo, podr√≠a ser de inter√©s comparar el valor de la funci√≥n minimizada, J( ÀÜŒ≤0, ÀÜŒ≤1), con nuestros estimadores y con los obtenidos con la funci√≥n lm.

```{r}
# Ajustar un modelo de regresi√≥n lineal con lm
lm_model <- lm(yi ~ xi)
lm_beta0 <- coef(lm_model)[1]
lm_beta1 <- coef(lm_model)[2]
lm_beta0
lm_beta1
```

