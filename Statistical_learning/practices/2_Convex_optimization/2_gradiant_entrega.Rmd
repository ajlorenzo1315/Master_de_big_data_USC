---
title: "M√©todo de descenso de gradiente en regresi√≥n"
author: "Alicia Jiajun Lorenzo, Abraham Trashorras "
date: "2023-10-16"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

0. Importamos las libreias necesarias

```{r}

```

1. Simular una muestra de tama√±o n = 100 de valores de (xi, yi), i ‚àà {1, . . . , n}.
a) Para ello, se  generan 100 observaciones xi de la uniforme (runif).

```{r}
n <- 100
xi <- runif(n)  # Genera 100 observaciones xi de la uniforme 

```

b) Los errores del modelo, i se  generan de la distribuci√≥n normal de media 0 y va-
rianza œÉ2. Por ejemplo, si cogemos œÉ2 = 1, esto se puede hacer con (rnorm(100,0,1)).

```{r}
sigma <- 1      # Variabilidad de los errores
epsilon <- rnorm(n, 0, sigma)  # Genera errores de la distribuci√≥n normal
```

c) Los valores de yi se calcular√≠an como yi = Œ≤0 + Œ≤1xi + i.
```{r}
beta0<-5 # supongamos que betha0=5 para el modelo
beta1<-3 # suponngamos que beta1=3 pare el modelo
yi <- beta0 + beta1 * xi + epsilon  # Calculamos los valores del  y
```
**NOTA** Comprobamos el comportamiento para distintas  variabilidades de los errores y betas

```{r}
# Crear un vector para almacenar las gr√°ficas
stds <- c(0.25, 0.5, 0.75, 1)

for (i in 1:length(stds)) {
  # Definimos sd
  # sd <- stds[i]  # Comentado para usar directamente stds[i]
  # Error (con desviaci√≥n t√≠pica sd)
  epsilon <- rnorm(n = n, sd = stds[i])
  # Calculamos y
  yi <- beta0 + beta1 * xi + epsilon
  # Ploteamos y lo almacenamos en la lista
  plot(xi, yi, main = paste('Diagrama de dispersi√≥n para: n¬∫ datos = ', n, '; sd = ', stds[i]))
}


```

**Nota** Mostramos la dispersi√≥n de los valores de nuestros datos simulados 
```{r}
plot(xi, yi, main = paste('Diagrama de dispersi√≥n para: ','n¬∫ datos =', n, '; sd = 1'))
```

2. Escoger la funci√≥n a minimizar. En este caso, la suma de los residuos al cuadrado:

  J(Œ≤0, Œ≤1) =‚àë(yi ‚àí Œ≤0 ‚àí Œ≤1xi)¬≤.

```{r cars}

suma_residuos_cuadrado <- function(beta0, beta1) {
  sum((yi - beta0 - beta1 * xi)^2)
}

```

3. Obtener las derivadas parciales de la funci√≥n a minimizar. A partir de estas, aplicad el algoritmo de m√©todo de descenso gradiente (v√©ase los apuntes del Tema de Optimizaci√≥n convexa). 

En la Secci√≥n 2.1 de la pr√°ctica de Modelos de regresi√≥n lineal con R se dan los pasos a se-
guir para el caso del modelo de regresi√≥n lineal simple


```{r pressure, echo=FALSE}

library(ggplot2)
library(gganimate)
# Aplicar el descenso de gradiente
learning_rate <- 0.01
num_iterations <- 1000
beta0 <- 0
beta1 <- 0


# Crear un dataframe para almacenar los resultados de cada iteraci√≥n
results <- data.frame(iteration = numeric(0), beta0 = numeric(0), beta1 = numeric(0), J = numeric(0))

# Realizar el descenso de gradiente y almacenar los resultados
for (iteration in 1:num_iterations) {
  dJ_beta0 <- -2 * sum(yi - beta0 - beta1 * xi)
  dJ_beta1 <- -2 * sum((yi - beta0 - beta1 * xi) * xi)
  beta0 <- beta0 - learning_rate * dJ_beta0
  beta1 <- beta1 - learning_rate * dJ_beta1
  J_value <- suma_residuos_cuadrado(beta0, beta1)
  results <- rbind(results, data.frame(iteration = iteration, beta0 = beta0, beta1 = beta1, J = J_value))
}
```

Crea una animaci√≥n utilizando ggplot2 y gganimate. Puedes utilizar un gr√°fico de dispersi√≥n para visualizar la convergencia del descenso de gradiente en cada iteraci√≥n.


p <- ggplot(results, aes(x = beta0, y = beta1)) +
  geom_point(aes(color = J), size = 2) +
  scale_color_gradient(low = "blue", high = "red") +
  labs(x = expression(beta[0]), y = expression(beta[1]), title = "Descenso del Gradiente") +
  transition_states(iteration, transition_length = 2, state_length = 1) +
  enter_fade() +
  exit_fade()

# Guardar la animaci√≥n en un archivo
anim_save("descenso_gradiente.gif", p)


4. Como en este caso, tenemos disponible la soluci√≥n √≥ptima a este problema en la funci√≥n lm. Podemos repetir (en un bucle) varias veces los pasos anteriores y hacer una comparativa con los estimadores ( ÀÜŒ≤0 y ÀÜŒ≤1) que devuelve dicha funci√≥n. Por ejemplo, podr√≠a ser de inter√©s comparar el valor de la funci√≥n minimizada, J( ÀÜŒ≤0, ÀÜŒ≤1), con nuestros estimadores y con los obtenidos con la funci√≥n lm.


# Ajustar un modelo de regresi√≥n lineal con lm
lm_model <- lm(yi ~ xi)
lm_beta0 <- coef(lm_model)[1]
lm_beta1 <- coef(lm_model)[2]
lm_beta0
lm_beta1


