---
title: "Método de descenso de gradiente en regresión"
author: "Alicia Jiajun Lorenzo, Abraham Trashorras "
date: "2023-10-16"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1. Simular una muestra de tamaño n = 100 de valores de (xi, yi), i $\in\{1,\ldots, n\}$
a) Para ello, se  generan 100 observaciones xi de la uniforme (runif).

```{r}
n <- 100
xi <- runif(n)  # Genera 100 observaciones xi de la uniforme 
```

b) Los errores del modelo, i se  generan de la distribución normal de media 0 y desviacion tipica $sigma$. Por ejemplo, si cogemos $\sigma = 1$, esto se puede hacer con (rnorm(100,0,1)).

```{r}
sigma <- 1      # Variabilidad de los errores
epsilon <- rnorm(n, 0, sigma)  # Genera errores de la distribución normal

```

c) Los valores de yi se calcularían como $y_i= \beta_0 + \beta_1 x_i + \epsilon_i$

```{r}
beta0<-5 # supongamos que betha0=5 para el modelo
beta1<-3 # suponngamos que beta1=3 pare el modelo
yi <- beta0 + beta1 * xi + epsilon  # Calculamos los valores del  y
```
**NOTA** Comprobamos el comportamiento para distintas  variabilidades de los errores y betas

```{r}
# Crear un vector para almacenar las gráficas

stds <- c(0.25, 0.5, 0.75, 1,2,3)
n <- 100  # Número de datos
beta0 <- 5
beta1 <- 3
xi <- rnorm(n)  # Datos xi generados aleatoriamente

# Configurar el diseño de subgráficos
par(mfrow = c(2, 3), mar = c(4, 4, 2, 2),oma = c(0, 0, 2, 0))  # Organizar en 2 filas y 2 columnas, ajustar márgenes

for (i in 1:length(stds)) {
  # Definimos sd
  # sd <- stds[i]  # Comentado para usar directamente stds[i]
  # Error (con desviación típica sd)
  epsilon <- rnorm(n = n, sd = stds[i])
  # Calculamos y
  yi <- beta0 + beta1 * xi + epsilon
  # Ploteamos y lo almacenamos en la lista
  plot(xi, yi, main = paste('nº datos = ', n, '; sd = ', stds[i]))
}

# Agregar un título general que englobe los subgráficos
mtext("Diagrama de dispersión para:", outer = TRUE, line = 0)

```

**Nota** Mostramos la dispersión que selecionamos de los valores de nuestros datos simulados  n

Para este trabajo usaremos una desviación tipica igual a 2  ya que genera suficiente dispersión para hacer las pruebas que deseamos 

```{r}
n <- 100  # Número de datos
beta0 <- 5
beta1 <- 3
xi <- rnorm(n)  # Datos xi generados aleatoriamente
epsilon <- rnorm(n = n, sd = 2)
# Calculamos y
yi <- beta0 + beta1 * xi + epsilon
```


2. Escoger la función a minimizar . En este caso, la suma de los residuos al cuadrado:

\[ J(\beta_0, \beta_1) = \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2 \]


```{r cars}
# Función a modelar
f <- function(x, beta0, beta1) {
return(beta0 + beta1 * x)
}

# Definir la función de costo J(beta0, beta1) 
#suma de residuis al cuadrado 
grad_costo <- function(y,x,beta0, beta1) {
  # Obtenemos la el error de valor y la prediccion
  error <- y - f(x,beta0,beta1)
  # Calculamos el gradiente de f(x)
  d_beta0 <- - sum(error) 
  d_beta1 <- - sum(error * x) 
  return(c(d_beta0, d_beta1))
}

# RSE (Residual Standard Error)
rse <- function(y,x,beta0,beta1) {
prediccion<-f(x,beta0,beta1)
n <- length(y)
rss <- sum((prediccion - y)^2)
rse <- sqrt((rss / (n - 2)))
return(rse)
}

```

3. Obtener las derivadas parciales de la función a minimizar . A partir de estas, aplicar el algoritmo de método de descenso gradiente.


```{r pressure, echo=FALSE}
# Función del descenso de gradiente
descenso_gradiente <- function(y, x, tasa_aprendizaje, num_iteraciones, tolerancia) {
  beta0_inicial <- 0
  beta1_inicial <- 0
  beta0_vector <- rep(0, times = num_iteraciones)
  converge=FALSE
  for (i in 1:num_iteraciones) {
    grad <- grad_costo(y,x,beta0_inicial, beta1_inicial)
    
    beta0_nuevo <- beta0_inicial - tasa_aprendizaje * grad[1]
    beta1_nuevo <- beta1_inicial - tasa_aprendizaje * grad[2]
    
    cambio_beta0 <- abs(beta0_nuevo - beta0_inicial)
    cambio_beta1 <- abs(beta1_nuevo - beta1_inicial)
    
    # Calculamos el rse cometido con la función de coste
    coste <- rse(y, x, beta0_nuevo, beta1_nuevo)
    
    
    beta0_inicial <- beta0_nuevo
    beta1_inicial <- beta1_nuevo
    beta0_vector[i]<-beta0_inicial
    
    # Comprobamos si se cumple el criterio de parada
    # if (sqrt(sum(grad^2)) < tolerancia) {
    #  converge=TRUE
    #  break
    #}
    
  }
  if (converge){
    print(paste("beta0: ",beta0_nuevo))
    print(paste("beta1: ",beta1_nuevo))
    print(paste("RSE: ", coste))
    print(paste("Iteracion en la que converge", i))
  }
  else{
    print(paste("beta0: ",beta0_nuevo))
    print(paste("beta1: ",beta1_nuevo))
    print(paste("RSE: ", coste))
    print(paste("NO CONVERGIO PARA EL NUMERO", i,"DE ITERACIONES"))
  }
  
  #print(paste("beta0: ",beta0_vector))
  resultado <- list(beta0_nuevo, beta1_nuevo,coste,converge,beta0_vector)
  return(resultado)
}

# Llamada a la función descenso_gradiente
tasa_aprendizaje <- 0.001
num_iteraciones <- 1000
tolerancia <- 0.0

betas_estimados <- descenso_gradiente(yi, xi, tasa_aprendizaje, num_iteraciones, tolerancia)


```
Compararemos mediante dos gráficas la recta de regresión obtenida por nuestro modelo y la obtenida mediante el uso de la función lm:

```{r}

par(mfrow = c(1, 2))

plot(xi, yi, main = "Método del gradiente")
lines(c(-10:10), f(c(-10:10), betas_estimados[[1]], betas_estimados[[2]]), col = "red", lwd = 1)

plot(xi, yi, main = "Función `lm`")
# Calculamos el modelo re regresión mediante `lm`
ml <- lm(yi ~ xi)
abline(ml, col = "red")

```
A simple vista parece que los resultados que aportan ambos metodos es similar, comprobaremos que valores nos da exactament cada método

Descenso de gradiente:
```{r}
print(paste("beta0: ",betas_estimados[[1]]))
print(paste("beta1: ",betas_estimados[[2]]))
print(paste("RSE: ", betas_estimados[[3]]))
```
Usando el método lm para modelos lineales:
```{r}

print(paste("beta0: ",ml$coefficients[[1]]))
print(paste("beta1: ",ml$coefficients[[2]]))
print(paste("RSE: ", sigma(ml)))
resumen(ml)
```
Como se observa númericamente tambien son similares   por lo cual podemos deducir que practicamente pueden dar el mismo resultado
