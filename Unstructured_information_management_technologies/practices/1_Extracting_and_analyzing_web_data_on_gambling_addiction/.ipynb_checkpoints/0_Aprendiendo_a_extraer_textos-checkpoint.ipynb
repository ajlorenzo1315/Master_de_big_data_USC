{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36c7d579",
   "metadata": {},
   "source": [
    "##  Instalamos las dependencias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "664f49b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Downloading Scrapy-2.9.0-py2.py3-none-any.whl (277 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.2/277.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting packaging\n",
      "  Using cached packaging-23.1-py3-none-any.whl (48 kB)\n",
      "Collecting PyDispatcher>=2.0.5\n",
      "  Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
      "Collecting protego>=0.1.15\n",
      "  Downloading Protego-0.3.0-py2.py3-none-any.whl (8.5 kB)\n",
      "Collecting zope.interface>=5.1.0\n",
      "  Downloading zope.interface-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.5/241.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting service-identity>=18.1.0\n",
      "  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting queuelib>=1.4.2\n",
      "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
      "Collecting cryptography>=3.4.6\n",
      "  Downloading cryptography-41.0.4-cp37-abi3-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/alourido/anaconda3/envs/TGINE/lib/python3.7/site-packages (from scrapy) (65.6.3)\n",
      "Collecting itemloaders>=1.0.1\n",
      "  Downloading itemloaders-1.1.0-py3-none-any.whl (11 kB)\n",
      "Collecting w3lib>=1.17.0\n",
      "  Downloading w3lib-2.1.2-py3-none-any.whl (21 kB)\n",
      "Collecting cssselect>=0.9.1\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting pyOpenSSL>=21.0.0\n",
      "  Downloading pyOpenSSL-23.2.0-py3-none-any.whl (59 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting itemadapter>=0.1.0\n",
      "  Downloading itemadapter-0.8.0-py3-none-any.whl (11 kB)\n",
      "Collecting parsel>=1.5.0\n",
      "  Downloading parsel-1.8.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting lxml>=4.3.0\n",
      "  Using cached lxml-4.9.3-cp37-cp37m-manylinux_2_28_x86_64.whl (7.4 MB)\n",
      "Collecting tldextract\n",
      "  Downloading tldextract-3.6.0-py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.4/97.4 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting Twisted>=18.9.0\n",
      "  Downloading twisted-23.8.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cffi>=1.12\n",
      "  Using cached cffi-1.15.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\n",
      "Collecting jmespath>=0.9.5\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting typing-extensions\n",
      "  Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Collecting six\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting attrs>=19.1.0\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting pyasn1-modules\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyasn1\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting automat>=0.8.0\n",
      "  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting incremental>=22.10.0\n",
      "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting constantly>=15.1\n",
      "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting filelock>=3.0.8\n",
      "  Downloading filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Collecting requests>=2.1.0\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting requests-file>=1.4\n",
      "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Collecting importlib-metadata\n",
      "  Using cached importlib_metadata-6.7.0-py3-none-any.whl (22 kB)\n",
      "Collecting pycparser\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (175 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.8/175.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/alourido/anaconda3/envs/TGINE/lib/python3.7/site-packages (from requests>=2.1.0->tldextract->scrapy) (2022.12.7)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.0.5-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting zipp>=0.5\n",
      "  Using cached zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Installing collected packages: PyDispatcher, incremental, constantly, zope.interface, zipp, w3lib, urllib3, typing-extensions, six, queuelib, pycparser, pyasn1, protego, packaging, lxml, jmespath, itemadapter, idna, filelock, cssselect, charset-normalizer, requests, pyasn1-modules, parsel, importlib-metadata, hyperlink, cffi, requests-file, itemloaders, cryptography, attrs, tldextract, service-identity, pyOpenSSL, automat, Twisted, scrapy\n",
      "Successfully installed PyDispatcher-2.0.7 Twisted-23.8.0 attrs-23.1.0 automat-22.10.0 cffi-1.15.1 charset-normalizer-3.2.0 constantly-15.1.0 cryptography-41.0.4 cssselect-1.2.0 filelock-3.12.2 hyperlink-21.0.0 idna-3.4 importlib-metadata-6.7.0 incremental-22.10.0 itemadapter-0.8.0 itemloaders-1.1.0 jmespath-1.0.1 lxml-4.9.3 packaging-23.1 parsel-1.8.1 protego-0.3.0 pyOpenSSL-23.2.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 pycparser-2.21 queuelib-1.6.2 requests-2.31.0 requests-file-1.5.1 scrapy-2.9.0 service-identity-21.1.0 six-1.16.0 tldextract-3.6.0 typing-extensions-4.7.1 urllib3-2.0.5 w3lib-2.1.2 zipp-3.15.0 zope.interface-6.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e0ca45",
   "metadata": {},
   "source": [
    "1. Crear un nuevo proyecto Scrapy:\n",
    "\n",
    "   Abre una terminal y navega a la ubicación donde quieras crear tu proyecto Scrapy. Luego, ejecuta el siguiente comando para crear un nuevo proyecto:\n",
    "   \n",
    "       \n",
    "   \n",
    "Para lanzarlos desde jupyter-notebook lanzamos la siguiente celda\n",
    "\n",
    "- Usar ! antes del comando:\n",
    "\n",
    "    Puedes ejecutar un solo comando de terminal en una celda de código utilizando el signo de exclamación (!) seguido del comando. Por ejemplo, para listar el contenido de un directorio, puedes hacer lo siguiente:\n",
    "\n",
    "- Usar % para comandos mágicos:\n",
    "\n",
    "    Jupyter Notebook también admite comandos mágicos que comienzan con %. Los comandos mágicos son comandos especiales que permiten realizar tareas específicas. Puedes usar % seguido del nombre del comando mágico para ejecutarlos. Por ejemplo, para listar el contenido de un directorio usando un comando mágico, puedes hacer lo siguiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3178aa56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'mynewsproject', using template directory '/home/alourido/anaconda3/envs/TGINE/lib/python3.7/site-packages/scrapy/templates/project', created in:\r\n",
      "    /home/alourido/Escritorio/Master_de_big_data_USC/Unstructured_information_management_technologies/practices/1_Extracting_and_analyzing_web_data_on_gambling_addiction/mynewsproject\r\n",
      "\r\n",
      "You can start your first spider with:\r\n",
      "    cd mynewsproject\r\n",
      "    scrapy genspider example example.com\r\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject mynewsproject\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df1cc8",
   "metadata": {},
   "source": [
    "2. Definir una araña:\n",
    "\n",
    "    - Navega al directorio del proyecto recién creado:\n",
    "        \n",
    "        cd mynewsproject\n",
    "\n",
    "\n",
    "    - Luego, crea una nueva araña web ejecutando el siguiente comando:\n",
    "\n",
    "\n",
    "        scrapy genspider example example.com\n",
    "\n",
    "- Esto generará un archivo llamado example.py en el directorio mynewsproject/spiders con una plantilla básica para tu araña."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e86f31a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created spider 'example' using template 'basic' in module:\r\n",
      "  mynewsproject.spiders.example\r\n"
     ]
    }
   ],
   "source": [
    "!cd mynewsproject && scrapy genspider example example.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f583eded",
   "metadata": {},
   "source": [
    "3. Editar la araña:\n",
    "\n",
    "Abre el archivo ./mynewsproject/mynewsproject/spiders/example.py en un editor de texto y edita el código para que se vea así:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class ExampleSpider(scrapy.Spider):\n",
    "    name = 'example'\n",
    "    start_urls = ['http://example.com']\n",
    "\n",
    "def parse(self, response):\n",
    "        for news in response.css('div.news-item'):\n",
    "            yield {\n",
    "                'title': news.css('h2::text').get(),\n",
    "                'link': news.css('a::attr(href)').get(),\n",
    "            }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88b4266",
   "metadata": {},
   "source": [
    "4. Ejecutar la araña:\n",
    "\n",
    "Para ejecutar la araña y obtener los datos, ejecuta el siguiente comando en la terminal, estando en el directorio del proyecto:\n",
    "\n",
    "  \n",
    "    scrapy crawl example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e51eee01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Twisted==22.10.0\n",
      "  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: constantly>=15.1 in /home/alourido/anaconda3/envs/TGINE/lib/python3.7/site-packages (from Twisted==22.10.0) (15.1.0)\n",
      "Requirement already satisfied: Automat>=0.8.0 in /home/alourido/anaconda3/envs/TGINE/lib/python3.7/site-packages (from Twisted==22.10.0) (22.10.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/alourido/anaconda3/envs/TGINE/lib/python3.7/site-packages (from Twisted==22.10.0) (23.1.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /home/alourido/anaconda3/envs/TGINE/lib/python3.7/site-packages (from Twisted==22.10.0) (21.0.0)\n",
      "Requirement already satisfied: zope.interface>=4.4.2 in /home/alourido/anaconda3/envs/TGINE/lib/python3.7/site-packages (from Twisted==22.10.0) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /home/alourido/anaconda3/envs/TGINE/lib/python3.7/site-packages (from Twisted==22.10.0) (4.7.1)\n",
      "Requirement already satisfied: incremental>=21.3.0 in /home/alourido/anaconda3/envs/TGINE/lib/python3.7/site-packages (from Twisted==22.10.0) (22.10.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/alourido/anaconda3/envs/TGINE/lib/python3.7/site-packages (from attrs>=19.2.0->Twisted==22.10.0) (6.7.0)\n",
      "Requirement already satisfied: six in /home/alourido/anaconda3/envs/TGINE/lib/python3.7/site-packages (from Automat>=0.8.0->Twisted==22.10.0) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.5 in /home/alourido/anaconda3/envs/TGINE/lib/python3.7/site-packages (from hyperlink>=17.1.1->Twisted==22.10.0) (3.4)\n",
      "Requirement already satisfied: setuptools in /home/alourido/anaconda3/envs/TGINE/lib/python3.7/site-packages (from zope.interface>=4.4.2->Twisted==22.10.0) (65.6.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/alourido/anaconda3/envs/TGINE/lib/python3.7/site-packages (from importlib-metadata->attrs>=19.2.0->Twisted==22.10.0) (3.15.0)\n",
      "Installing collected packages: Twisted\n",
      "  Attempting uninstall: Twisted\n",
      "    Found existing installation: Twisted 23.8.0\n",
      "    Uninstalling Twisted-23.8.0:\n",
      "      Successfully uninstalled Twisted-23.8.0\n",
      "Successfully installed Twisted-22.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install Twisted==22.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7f7cd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-29 16:45:12 [scrapy.utils.log] INFO: Scrapy 2.9.0 started (bot: mynewsproject)\n",
      "2023-09-29 16:45:12 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.7.16 (default, Jan 17 2023, 22:20:44) - [GCC 11.2.0], pyOpenSSL 23.2.0 (OpenSSL 3.1.3 19 Sep 2023), cryptography 41.0.4, Platform Linux-6.2.0-33-generic-x86_64-with-debian-bookworm-sid\n",
      "2023-09-29 16:45:12 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'BOT_NAME': 'mynewsproject',\n",
      " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
      " 'NEWSPIDER_MODULE': 'mynewsproject.spiders',\n",
      " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
      " 'ROBOTSTXT_OBEY': True,\n",
      " 'SPIDER_MODULES': ['mynewsproject.spiders'],\n",
      " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
      "2023-09-29 16:45:12 [asyncio] DEBUG: Using selector: EpollSelector\n",
      "2023-09-29 16:45:12 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
      "2023-09-29 16:45:12 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
      "2023-09-29 16:45:12 [scrapy.extensions.telnet] INFO: Telnet Password: 0c7b97f6f1aad355\n",
      "2023-09-29 16:45:12 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-09-29 16:45:12 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-09-29 16:45:12 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-09-29 16:45:12 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-09-29 16:45:12 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-09-29 16:45:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-09-29 16:45:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2023-09-29 16:45:13 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://example.com/robots.txt> (referer: None)\n",
      "2023-09-29 16:45:13 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.\n",
      "2023-09-29 16:45:13 [protego] DEBUG: Rule at line 13 without any user agent to enforce it on.\n",
      "2023-09-29 16:45:13 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.\n",
      "2023-09-29 16:45:13 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.\n",
      "2023-09-29 16:45:13 [protego] DEBUG: Rule at line 19 without any user agent to enforce it on.\n",
      "2023-09-29 16:45:13 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.\n",
      "2023-09-29 16:45:13 [protego] DEBUG: Rule at line 22 without any user agent to enforce it on.\n",
      "2023-09-29 16:45:13 [protego] DEBUG: Rule at line 23 without any user agent to enforce it on.\n",
      "2023-09-29 16:45:13 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.\n",
      "2023-09-29 16:45:13 [protego] DEBUG: Rule at line 27 without any user agent to enforce it on.\n",
      "2023-09-29 16:45:13 [protego] DEBUG: Rule at line 29 without any user agent to enforce it on.\n",
      "2023-09-29 16:45:13 [protego] DEBUG: Rule at line 31 without any user agent to enforce it on.\n",
      "2023-09-29 16:45:13 [protego] DEBUG: Rule at line 32 without any user agent to enforce it on.\n",
      "2023-09-29 16:45:13 [protego] DEBUG: Rule at line 43 without any user agent to enforce it on.\n",
      "2023-09-29 16:45:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://example.com> (referer: None)\n",
      "2023-09-29 16:45:13 [scrapy.core.scraper] ERROR: Spider error processing <GET http://example.com> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alourido/anaconda3/envs/TGINE/lib/python3.7/site-packages/twisted/internet/defer.py\", line 893, in _runCallbacks\n",
      "    current.result, *args, **kwargs\n",
      "  File \"/home/alourido/anaconda3/envs/TGINE/lib/python3.7/site-packages/scrapy/spiders/__init__.py\", line 73, in _parse\n",
      "    return self.parse(response, **kwargs)\n",
      "  File \"/home/alourido/anaconda3/envs/TGINE/lib/python3.7/site-packages/scrapy/spiders/__init__.py\", line 77, in parse\n",
      "    f\"{self.__class__.__name__}.parse callback is not defined\"\n",
      "NotImplementedError: ExampleSpider.parse callback is not defined\n",
      "2023-09-29 16:45:13 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-09-29 16:45:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 432,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 2012,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'downloader/response_status_count/404': 1,\n",
      " 'elapsed_time_seconds': 0.598641,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 9, 29, 14, 45, 13, 472676),\n",
      " 'httpcompression/response_bytes': 2512,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'log_count/DEBUG': 19,\n",
      " 'log_count/ERROR': 1,\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 75378688,\n",
      " 'memusage/startup': 75378688,\n",
      " 'response_received_count': 2,\n",
      " 'robotstxt/request_count': 1,\n",
      " 'robotstxt/response_count': 1,\n",
      " 'robotstxt/response_status_count/404': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'spider_exceptions/NotImplementedError': 1,\n",
      " 'start_time': datetime.datetime(2023, 9, 29, 14, 45, 12, 874035)}\n",
      "2023-09-29 16:45:13 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!cd mynewsproject && scrapy crawl example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef2309c",
   "metadata": {},
   "source": [
    "5. para almacenar los datos extraídos en formato JSON, puedes hacer lo siguiente:\n",
    "\n",
    "    En la misma araña que definiste anteriormente (el archivo example.py), importa el módulo json al principio del archivo . Y modifica la función parse para que almacene los datos en un archivo JSON. Puedes hacerlo de la siguiente manera:\n",
    " ```python  \n",
    "import scrapy\n",
    "import json\n",
    "\n",
    "class ExampleSpider(scrapy.Spider):\n",
    "    name = 'example'\n",
    "    start_urls = ['http://example.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "            data = []  # Creamos una lista para almacenar los datos\n",
    "\n",
    "            for news in response.css('div.news-item'):\n",
    "                item = {\n",
    "                    'title': news.css('h2::text').get(),\n",
    "                    'link': news.css('a::attr(href)').get(),\n",
    "                }\n",
    "                data.append(item)  # Agregamos cada item a la lista\n",
    "\n",
    "            # Al final de la función, almacenamos los datos en un archivo JSON\n",
    "            with open('output.json', 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(data, json_file, ensure_ascii=False, indent=2)\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b22d4e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-29 16:50:08 [scrapy.utils.log] INFO: Scrapy 2.9.0 started (bot: mynewsproject)\n",
      "2023-09-29 16:50:08 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.7.16 (default, Jan 17 2023, 22:20:44) - [GCC 11.2.0], pyOpenSSL 23.2.0 (OpenSSL 3.1.3 19 Sep 2023), cryptography 41.0.4, Platform Linux-6.2.0-33-generic-x86_64-with-debian-bookworm-sid\n",
      "2023-09-29 16:50:08 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'BOT_NAME': 'mynewsproject',\n",
      " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
      " 'NEWSPIDER_MODULE': 'mynewsproject.spiders',\n",
      " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
      " 'ROBOTSTXT_OBEY': True,\n",
      " 'SPIDER_MODULES': ['mynewsproject.spiders'],\n",
      " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
      "2023-09-29 16:50:08 [asyncio] DEBUG: Using selector: EpollSelector\n",
      "2023-09-29 16:50:08 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
      "2023-09-29 16:50:08 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
      "2023-09-29 16:50:08 [scrapy.extensions.telnet] INFO: Telnet Password: 03f50f0779bf23c7\n",
      "2023-09-29 16:50:08 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-09-29 16:50:09 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-09-29 16:50:09 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-09-29 16:50:09 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-09-29 16:50:09 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-09-29 16:50:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-09-29 16:50:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2023-09-29 16:50:09 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://example.com/robots.txt> (referer: None)\n",
      "2023-09-29 16:50:09 [protego] DEBUG: Rule at line 12 without any user agent to enforce it on.\n",
      "2023-09-29 16:50:09 [protego] DEBUG: Rule at line 13 without any user agent to enforce it on.\n",
      "2023-09-29 16:50:09 [protego] DEBUG: Rule at line 14 without any user agent to enforce it on.\n",
      "2023-09-29 16:50:09 [protego] DEBUG: Rule at line 18 without any user agent to enforce it on.\n",
      "2023-09-29 16:50:09 [protego] DEBUG: Rule at line 19 without any user agent to enforce it on.\n",
      "2023-09-29 16:50:09 [protego] DEBUG: Rule at line 20 without any user agent to enforce it on.\n",
      "2023-09-29 16:50:09 [protego] DEBUG: Rule at line 22 without any user agent to enforce it on.\n",
      "2023-09-29 16:50:09 [protego] DEBUG: Rule at line 23 without any user agent to enforce it on.\n",
      "2023-09-29 16:50:09 [protego] DEBUG: Rule at line 25 without any user agent to enforce it on.\n",
      "2023-09-29 16:50:09 [protego] DEBUG: Rule at line 27 without any user agent to enforce it on.\n",
      "2023-09-29 16:50:09 [protego] DEBUG: Rule at line 29 without any user agent to enforce it on.\n",
      "2023-09-29 16:50:09 [protego] DEBUG: Rule at line 31 without any user agent to enforce it on.\n",
      "2023-09-29 16:50:09 [protego] DEBUG: Rule at line 32 without any user agent to enforce it on.\n",
      "2023-09-29 16:50:09 [protego] DEBUG: Rule at line 43 without any user agent to enforce it on.\n",
      "2023-09-29 16:50:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://example.com> (referer: None)\n",
      "2023-09-29 16:50:09 [scrapy.core.scraper] ERROR: Spider error processing <GET http://example.com> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alourido/anaconda3/envs/TGINE/lib/python3.7/site-packages/twisted/internet/defer.py\", line 893, in _runCallbacks\n",
      "    current.result, *args, **kwargs\n",
      "  File \"/home/alourido/anaconda3/envs/TGINE/lib/python3.7/site-packages/scrapy/spiders/__init__.py\", line 73, in _parse\n",
      "    return self.parse(response, **kwargs)\n",
      "  File \"/home/alourido/anaconda3/envs/TGINE/lib/python3.7/site-packages/scrapy/spiders/__init__.py\", line 77, in parse\n",
      "    f\"{self.__class__.__name__}.parse callback is not defined\"\n",
      "NotImplementedError: ExampleSpider.parse callback is not defined\n",
      "2023-09-29 16:50:09 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-09-29 16:50:09 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 432,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 2012,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'downloader/response_status_count/404': 1,\n",
      " 'elapsed_time_seconds': 0.596979,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 9, 29, 14, 50, 9, 691535),\n",
      " 'httpcompression/response_bytes': 2512,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'log_count/DEBUG': 19,\n",
      " 'log_count/ERROR': 1,\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 75386880,\n",
      " 'memusage/startup': 75386880,\n",
      " 'response_received_count': 2,\n",
      " 'robotstxt/request_count': 1,\n",
      " 'robotstxt/response_count': 1,\n",
      " 'robotstxt/response_status_count/404': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'spider_exceptions/NotImplementedError': 1,\n",
      " 'start_time': datetime.datetime(2023, 9, 29, 14, 50, 9, 94556)}\n",
      "2023-09-29 16:50:09 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!cd mynewsproject && scrapy crawl example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4e464e",
   "metadata": {},
   "source": [
    "Almacenamiento de datos usando Feed exports¶\n",
    "\n",
    "One of the most frequently required features when implementing scrapers is being able to store the scraped data properly and, quite often, that means generating an “export file” with the scraped data (commonly called “export feed”) to be consumed by other systems.\n",
    "\n",
    "Scrapy provides this functionality out of the box with the Feed Exports, which allows you to generate feeds with the scraped items, using multiple serialization formats and storage backends\n",
    "\n",
    "example.py\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class ExampleSpider(scrapy.Spider):\n",
    "    name = 'example'\n",
    "    start_urls = ['http://example.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for news in response.css('div.news-item'):\n",
    "            yield {\n",
    "                'title': news.css('h2::text').get(),\n",
    "                'link': news.css('a::attr(href)').get(),\n",
    "            }\n",
    " ```\n",
    " \n",
    " 1. Ahora, en la misma clase ExampleSpider, configura cómo deseas exportar los datos utilizando la opción de \"FEED_EXPORTS\" en el archivo de configuración settings.py. Puedes configurar Scrapy para exportar los datos en formato JSON agregando el siguiente código en settings.py:\n",
    " \n",
    "```python\n",
    "FEED_FORMAT = 'json'\n",
    "FEED_URI = 'output.json'\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Esto configura Scrapy para exportar los datos en formato JSON y guardarlos en un archivo llamado output.json en el directorio de tu proyecto.\n",
    "\n",
    "Scrapy extraerá los datos y los exportará automáticamente en formato JSON al archivo output.json.\n",
    "\n",
    "Utilizar \"Feed Exports\" es una forma conveniente y sencilla de exportar datos en Scrapy, ya que Scrapy se encarga de la escritura y el formato del archivo de salida por ti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36761c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-29 16:59:52 [scrapy.utils.log] INFO: Scrapy 2.9.0 started (bot: mynewsproject)\n",
      "2023-09-29 16:59:52 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.7.16 (default, Jan 17 2023, 22:20:44) - [GCC 11.2.0], pyOpenSSL 23.2.0 (OpenSSL 3.1.3 19 Sep 2023), cryptography 41.0.4, Platform Linux-6.2.0-33-generic-x86_64-with-debian-bookworm-sid\n",
      "2023-09-29 16:59:52 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'BOT_NAME': 'mynewsproject',\n",
      " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
      " 'NEWSPIDER_MODULE': 'mynewsproject.spiders',\n",
      " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
      " 'ROBOTSTXT_OBEY': True,\n",
      " 'SPIDER_MODULES': ['mynewsproject.spiders'],\n",
      " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
      "2023-09-29 16:59:52 [asyncio] DEBUG: Using selector: EpollSelector\n",
      "2023-09-29 16:59:52 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
      "2023-09-29 16:59:52 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
      "2023-09-29 16:59:52 [scrapy.extensions.telnet] INFO: Telnet Password: de6c74ed7538cbeb\n",
      "2023-09-29 16:59:52 [py.warnings] WARNING: /home/alourido/anaconda3/envs/TGINE/lib/python3.7/site-packages/scrapy/extensions/feedexport.py:326: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
      "  exporter = cls(crawler)\n",
      "\n",
      "2023-09-29 16:59:52 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-09-29 16:59:52 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-09-29 16:59:52 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-09-29 16:59:52 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-09-29 16:59:52 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-09-29 16:59:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-09-29 16:59:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2023-09-29 16:59:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.boredapi.com/robots.txt> (referer: None)\n",
      "2023-09-29 16:59:52 [protego] DEBUG: Rule at line 1 without any user agent to enforce it on.\n",
      "2023-09-29 16:59:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.boredapi.com/api/activity> (referer: None)\n",
      "2023-09-29 16:59:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.boredapi.com/api/activity>\n",
      "{'Activity': 'Catch up with a friend over a lunch date', 'Type': 'social', 'Participants': 2}\n",
      "2023-09-29 16:59:53 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-09-29 16:59:53 [scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: output.json\n",
      "2023-09-29 16:59:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 454,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 1908,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 0.855946,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 9, 29, 14, 59, 53, 132951),\n",
      " 'item_scraped_count': 1,\n",
      " 'log_count/DEBUG': 7,\n",
      " 'log_count/INFO': 11,\n",
      " 'log_count/WARNING': 1,\n",
      " 'memusage/max': 75395072,\n",
      " 'memusage/startup': 75395072,\n",
      " 'response_received_count': 2,\n",
      " 'robotstxt/request_count': 1,\n",
      " 'robotstxt/response_count': 1,\n",
      " 'robotstxt/response_status_count/200': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2023, 9, 29, 14, 59, 52, 277005)}\n",
      "2023-09-29 16:59:53 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!cd mynewsproject && scrapy crawl example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d12863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat output.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
